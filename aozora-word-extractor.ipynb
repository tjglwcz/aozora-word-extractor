{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aozora Bunko Vocabulary Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aim of this project is to create dataframes based on vocabulary from classic Japanese literature available on Aozora Bunko, with relevant JLPT level data to estimate the difficulty of the book. The dataframes will later be used to create visual guides to Japanese literature based on their level of difficulty.\n",
    "\n",
    "[Aozora Bunko](https://www.aozora.gr.jp/) is a digital library that hosts classic Japanese literature in a convenient HTML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "The main libraries used in this project are\n",
    "- Janome - Japanese text processing, such as tokenisation and conjugation\n",
    "- BeautifulSoup - scraping text from HTML files\n",
    "- pandas - data frame creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source text preparation\n",
    "For this project I chose to analyze \"Rashōmon\" by Akutagawa Ryūnosuke. The original text can be accessed via the [link below](https://www.aozora.gr.jp/cards/000879/files/128_15261.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.aozora.gr.jp/cards/000879/files/128_15261.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assigning the Aozora Bunko link to a variable, I parse the text using BeautifulSoup. I also access the title of the book and the name of the author to name the csv file afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book loaded: 羅生門 by 芥川龍之介\n"
     ]
    }
   ],
   "source": [
    "book_source = requests.get(URL)\n",
    "soup = BeautifulSoup(book_source.content, \"html.parser\")\n",
    "book_text = soup.find(\"div\",{\"class\":\"main_text\"}).get_text()\n",
    "book_title = soup.find(\"h1\").get_text()\n",
    "book_author = soup.find(\"h2\").get_text()\n",
    "file_name = f'{book_author} - {book_title}.csv'\n",
    "print(f'Book loaded: {book_title} by {book_author}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataframe\n",
    "Next, I created a function that tokenises the text and creates an array of words. The function filters the text to retrieve only the most important parts of speech, such as nouns, adjectives and verbs. All words are converted to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordlist(text):\n",
    "    t = Tokenizer()\n",
    "    wordlist = []\n",
    "\n",
    "    for token in t.tokenize(text):\n",
    "        if \"名詞\" in token.part_of_speech or (token.part_of_speech.startswith(\"動詞\") and not token.part_of_speech.startswith(\"助動詞\")) or \"形容詞\" in token.part_of_speech or \"代名詞\" in token.part_of_speech:\n",
    "            wordlist.append(token.base_form)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating an array, I use a counter to consolidate the results and provide a number of occurrences of each word in the text. I create a dataframe from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>する</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ゐる</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>下人</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>事</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>云</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>惧</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>人目</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>患</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>雨風</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>九月</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "31    する     78\n",
       "11    ゐる     59\n",
       "5     下人     44\n",
       "2      事     31\n",
       "42     云     30\n",
       "..   ...    ...\n",
       "301    惧      1\n",
       "300   人目      1\n",
       "299    患      1\n",
       "298   雨風      1\n",
       "688   九月      1\n",
       "\n",
       "[689 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = extract_wordlist(book_text)\n",
    "wordlist = Counter(wordlist)\n",
    "wordlist = pd.DataFrame(wordlist.items(), columns = ['Word', 'Count']).sort_values(by = 'Count', ascending = False)\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the table above, Rashōmon consists of 689 unique words. The most common word in the story is the verb する (suru)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Tests\n",
    "To retrieve data on the JLPT level of each word, I use the [JLPT-VOCAB API](https://github.com/wkei/jlpt-vocab-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 1,\n",
       " 'offset': 0,\n",
       " 'limit': 10,\n",
       " 'words': [{'word': '人目',\n",
       "   'meaning': 'glimpse; public gaze',\n",
       "   'furigana': 'じんもく',\n",
       "   'romaji': 'jinmoku',\n",
       "   'level': 1}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://jlpt-vocab-api.vercel.app/api/words?word=人目\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the API response above, we can access not only the JLPT level of the word, but also its English definition and furigana spelling. I will not use this data, however, as the janome library is also able to provide the furigana spelling, regardless of whether there is a record in the API for the queried word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://jlpt-vocab-api.vercel.app/api/words?word=人目\").json()['words'][0]['level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request Functions\n",
    "I decided to split the API request into two separate functions. Previously I wanted to use the jisho.org API, but it tended to timeout frequently, so I implemented the make_api_request() function to automatically resend the request each time there was a timeout. Eventually I decided to change the API as the constant timeouts made the fetching process significantly longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(word):\n",
    "    url = f\"https://jlpt-vocab-api.vercel.app/api/words?word={word}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except (requests.exceptions.RequestException, requests.exceptions.Timeout):\n",
    "        print(f\"Timeout for word: {word}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        return make_api_request(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_info(wordlist):\n",
    "    readings = []\n",
    "    levels = []\n",
    "    t = Tokenizer()\n",
    "\n",
    "    for word in tqdm(wordlist, desc= 'Processing'):\n",
    "        response = make_api_request(word)\n",
    "        tokens = list(t.tokenize(word))\n",
    "\n",
    "        reading = tokens[0].reading\n",
    "        \n",
    "        try:\n",
    "            level = response['words'][0]['level']\n",
    "        except (IndexError, KeyError):\n",
    "            level = 'unknown'\n",
    "\n",
    "        readings.append(reading)\n",
    "        levels.append(level)\n",
    "\n",
    "    return readings, levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request Test\n",
    "After creating the functions, I tested them using a short vocabulary list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['キク', 'ヤク', 'ホンキ']\n",
      "[5, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testlist = ['聞く', '焼く', '本気']\n",
    "readings, levels = get_word_info(testlist)\n",
    "print(readings)\n",
    "print(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual vocabulary list creation\n",
    "My functions are working correctly and I am now able to populate my Rashōmon data frame with readings and information about JLPT levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 689/689 [06:47<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "wordlist['Reading'], wordlist['JLPT Level'] = get_word_info(wordlist['Word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I can export the list for further visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist.to_csv(file_name, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
