{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.aozora.gr.jp/cards/000879/files/128_15261.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book loaded: 羅生門 by 芥川龍之介\n"
     ]
    }
   ],
   "source": [
    "book_source = requests.get(URL)\n",
    "soup = BeautifulSoup(book_source.content, \"html.parser\")\n",
    "book_text = soup.find(\"div\",{\"class\":\"main_text\"}).get_text()\n",
    "book_title = soup.find(\"h1\").get_text()\n",
    "book_author = soup.find(\"h2\").get_text()\n",
    "file_name = f'{book_author} - {book_title}.csv'\n",
    "print(f'Book loaded: {book_title} by {book_author}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordlist(text):\n",
    "    t = Tokenizer()\n",
    "    wordlist = []\n",
    "\n",
    "    for token in t.tokenize(text):\n",
    "        if \"名詞\" in token.part_of_speech or (token.part_of_speech.startswith(\"動詞\") and not token.part_of_speech.startswith(\"助動詞\")) or \"形容詞\" in token.part_of_speech or \"代名詞\" in token.part_of_speech:\n",
    "            wordlist.append(token.base_form)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Word  Count\n",
      "31    する     78\n",
      "11    ゐる     59\n",
      "5     下人     44\n",
      "2      事     31\n",
      "42     云     30\n",
      "..   ...    ...\n",
      "301    惧      1\n",
      "300   人目      1\n",
      "299    患      1\n",
      "298   雨風      1\n",
      "688   九月      1\n",
      "\n",
      "[689 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "wordlist = extract_wordlist(book_text)\n",
    "wordlist = Counter(wordlist)\n",
    "wordlist = pd.DataFrame(wordlist.items(), columns = ['Word', 'Count']).sort_values(by = 'Count', ascending = False)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 1,\n",
       " 'offset': 0,\n",
       " 'limit': 10,\n",
       " 'words': [{'word': '人目',\n",
       "   'meaning': 'glimpse; public gaze',\n",
       "   'furigana': 'じんもく',\n",
       "   'romaji': 'jinmoku',\n",
       "   'level': 1}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://jlpt-vocab-api.vercel.app/api/words?word=人目\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://jlpt-vocab-api.vercel.app/api/words?word=人目\").json()['words'][0]['level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(word):\n",
    "    url = f\"https://jlpt-vocab-api.vercel.app/api/words?word={word}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except (requests.exceptions.RequestException, requests.exceptions.Timeout):\n",
    "        print(f\"Timeout for word: {word}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        return make_api_request(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_info(wordlist):\n",
    "    readings = []\n",
    "    levels = []\n",
    "    t = Tokenizer()\n",
    "\n",
    "    for word in wordlist:\n",
    "        response = make_api_request(word)\n",
    "        tokens = list(t.tokenize(word))\n",
    "\n",
    "        reading = tokens[0].reading\n",
    "        \n",
    "        try:\n",
    "            level = response['words'][0]['level']\n",
    "        except (IndexError, KeyError):\n",
    "            level = 'unknown'\n",
    "\n",
    "        readings.append(reading)\n",
    "        levels.append(level)\n",
    "\n",
    "    return readings, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['キク', 'ヤク', 'ホンキ']\n",
      "[5, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "testlist = ['聞く', '焼く', '本気']\n",
    "readings, levels = get_word_info(testlist)\n",
    "print(readings)\n",
    "print(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist['Reading'], wordlist['Level'] = get_word_info(wordlist['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist.to_csv(file_name, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
