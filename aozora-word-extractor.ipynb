{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aozora Bunko Vocabulary Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The goal of this project is to create dataframes based on vocabulary from Japanese classic literature available on Aozora Bunko, with relevant JLPT level data to estimate the difficulty of the book. Dataframes will be later used for creating visual guides to Japanese literature, based on their difficulty.\n",
    "\n",
    "[Aozora Bunko](https://www.aozora.gr.jp/) is a digital library hosting classic Japanese literature in a convenient HTML format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "The main libraries used for this project are:\n",
    "- Janome - processing Japanese text, such as tokenizing and conjugation\n",
    "- BeautifulSoup - scraping text from HTML files\n",
    "- pandas - dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source text preparation\n",
    "For this project, I chose \"Rashōmon\" by Ryūnosuke Akutagawa. The original text can be accessed by the [link provided below](https://www.aozora.gr.jp/cards/000879/files/128_15261.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.aozora.gr.jp/cards/000879/files/128_15261.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assigning the Aozora Bunko link to a variable, I am parsing the text using BeautifulSoup. I am also accessing the book's title and author's name to name the csv file afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book loaded: 羅生門 by 芥川龍之介\n"
     ]
    }
   ],
   "source": [
    "book_source = requests.get(URL)\n",
    "soup = BeautifulSoup(book_source.content, \"html.parser\")\n",
    "book_text = soup.find(\"div\",{\"class\":\"main_text\"}).get_text()\n",
    "book_title = soup.find(\"h1\").get_text()\n",
    "book_author = soup.find(\"h2\").get_text()\n",
    "file_name = f'{book_author} - {book_title}.csv'\n",
    "print(f'Book loaded: {book_title} by {book_author}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataframe\n",
    "Next, I created a function that tokenizes the text and creates an array of words. The function filters the text to retrieve only the most important parts of speech, such as nouns, adjectives and verbs. All words are converted to its base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordlist(text):\n",
    "    t = Tokenizer()\n",
    "    wordlist = []\n",
    "\n",
    "    for token in t.tokenize(text):\n",
    "        if \"名詞\" in token.part_of_speech or (token.part_of_speech.startswith(\"動詞\") and not token.part_of_speech.startswith(\"助動詞\")) or \"形容詞\" in token.part_of_speech or \"代名詞\" in token.part_of_speech:\n",
    "            wordlist.append(token.base_form)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating an array, I am using a counter to consolidate the results and provide a number of occurences of every word in the text. I am crating a dataframe from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>する</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ゐる</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>下人</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>事</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>云</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>惧</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>人目</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>患</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>雨風</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>九月</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "31    する     78\n",
       "11    ゐる     59\n",
       "5     下人     44\n",
       "2      事     31\n",
       "42     云     30\n",
       "..   ...    ...\n",
       "301    惧      1\n",
       "300   人目      1\n",
       "299    患      1\n",
       "298   雨風      1\n",
       "688   九月      1\n",
       "\n",
       "[689 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = extract_wordlist(book_text)\n",
    "wordlist = Counter(wordlist)\n",
    "wordlist = pd.DataFrame(wordlist.items(), columns = ['Word', 'Count']).sort_values(by = 'Count', ascending = False)\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the table above, Rashōmon consists of 689 unique words. The most used word in the story is a verb する (suru)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Tests\n",
    "For retrieving data about JLPT level of each word, I am using [JLPT-VOCAB API](https://github.com/wkei/jlpt-vocab-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 1,\n",
       " 'offset': 0,\n",
       " 'limit': 10,\n",
       " 'words': [{'word': '人目',\n",
       "   'meaning': 'glimpse; public gaze',\n",
       "   'furigana': 'じんもく',\n",
       "   'romaji': 'jinmoku',\n",
       "   'level': 1}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://jlpt-vocab-api.vercel.app/api/words?word=人目\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the API response above, we can access not only the JLPT level of the word, but also its english definition and furigana spelling. However, I am not going to use this data, since janome library is also capable of providing the furigana spelling regardless if there is an record in the API for the queried word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://jlpt-vocab-api.vercel.app/api/words?word=人目\").json()['words'][0]['level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request Functions\n",
    "I have decided to split the API fetching into two separate functions. Previously, I wanted to use the jisho.org API, but it tends to timeout frequently, thus I created the make_api_request() function to automatically resend the request anytime there was a timeout. Eventually, I have decided to change the API, since constant timeouts made the fetching process significantly longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(word):\n",
    "    url = f\"https://jlpt-vocab-api.vercel.app/api/words?word={word}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except (requests.exceptions.RequestException, requests.exceptions.Timeout):\n",
    "        print(f\"Timeout for word: {word}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        return make_api_request(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_info(wordlist):\n",
    "    readings = []\n",
    "    levels = []\n",
    "    t = Tokenizer()\n",
    "\n",
    "    for word in tqdm(wordlist, desc= 'Processing'):\n",
    "        response = make_api_request(word)\n",
    "        tokens = list(t.tokenize(word))\n",
    "\n",
    "        reading = tokens[0].reading\n",
    "        \n",
    "        try:\n",
    "            level = response['words'][0]['level']\n",
    "        except (IndexError, KeyError):\n",
    "            level = 'unknown'\n",
    "\n",
    "        readings.append(reading)\n",
    "        levels.append(level)\n",
    "\n",
    "    return readings, levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request Test\n",
    "After creating the functions, I have tested them out by using a short word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['キク', 'ヤク', 'ホンキ']\n",
      "[5, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testlist = ['聞く', '焼く', '本気']\n",
    "readings, levels = get_word_info(testlist)\n",
    "print(readings)\n",
    "print(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual vocabulary list creation\n",
    "My functions are working correctly, so now I am able to populate my Rashōmon dataframe with readings and information about JLPT levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist['Reading'], wordlist['JLPT Level'] = get_word_info(wordlist['Word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, I am able to export the list for further visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist.to_csv(file_name, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
